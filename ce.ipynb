{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from time import perf_counter\n",
    "import einops\n",
    "\n",
    "from typing import Literal\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2CELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=-1)\n",
    "        self.l2 = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, output: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        labels_argmax = labels.argmax(dim=-1, keepdim=True)\n",
    "        labels_onehot = F.one_hot(labels_argmax.squeeze(), num_classes=labels.shape[-1]).float().squeeze()\n",
    "\n",
    "        # TODO: reduction=\"none\", then manually reduce?\n",
    "        loss = self.ce(output, labels_argmax.squeeze()) * self.l2(output, labels) / self.l2(output, labels_onehot)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_l2ce_loss():\n",
    "    b, s, d = 16, 16, 512\n",
    "    loss_fn = L2CELoss()\n",
    "\n",
    "    x = torch.randn(b, s, d)\n",
    "    y = F.one_hot(torch.randint(0, d, (b, s)), num_classes=d).float()\n",
    "    num_iter = 50\n",
    "\n",
    "    losses = []\n",
    "    for alpha in range(num_iter):\n",
    "        alpha = alpha / num_iter\n",
    "        target = torch.lerp(x, y, alpha)\n",
    "        losses.append(loss_fn(x.flatten(0, 1), target.flatten(0, 1)).item())\n",
    "    print(x.flatten(0, 1).shape)\n",
    "    assert losses == sorted(losses)\n",
    "    assert losses[0] == 0.0\n",
    "\n",
    "    # Check out that this is equivalent to the CE loss if we give a 1hot target\n",
    "    ce = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=-1)\n",
    "\n",
    "    squashed_y = y.view(-1, d).argmax(dim=-1).squeeze(())\n",
    "    ce_loss = ce(x.view(-1, d), squashed_y)\n",
    "    l2ce_loss = loss_fn(x.view(-1, d), y.view(-1, d))\n",
    "    assert torch.allclose(l2ce_loss, ce_loss)\n",
    "\n",
    "\n",
    "test_l2ce_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d=256, loss=tensor(5.8629, grad_fn=<DivBackward0>), 0.060s\n",
      "d=512, loss=tensor(6.6182, grad_fn=<DivBackward0>), 0.094s\n",
      "d=768, loss=tensor(6.9890, grad_fn=<DivBackward0>), 0.097s\n",
      "d=1024, loss=tensor(7.3310, grad_fn=<DivBackward0>), 0.151s\n",
      "d=1280, loss=tensor(7.5418, grad_fn=<DivBackward0>), 0.139s\n",
      "d=1536, loss=tensor(7.7343, grad_fn=<DivBackward0>), 0.208s\n",
      "d=1792, loss=tensor(7.8542, grad_fn=<DivBackward0>), 0.259s\n",
      "d=2048, loss=tensor(8.0103, grad_fn=<DivBackward0>), 0.638s\n",
      "d=2304, loss=tensor(8.1578, grad_fn=<DivBackward0>), 0.433s\n"
     ]
    }
   ],
   "source": [
    "b, s, d_init = 16, 16, 256\n",
    "l2 = nn.MSELoss()\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "for d_prod in range(1, 10):\n",
    "    d = d_prod * d_init\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    x = torch.randn(b, s, d)\n",
    "    target = torch.randint(0, d, (b, s))\n",
    "    target_onehot = F.one_hot(target, num_classes=d).float()\n",
    "    noisy_target_onehot = target_onehot + torch.randn_like(target_onehot) * 0.1\n",
    "\n",
    "    model = nn.Linear(d, d)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    loss_ce = ce(model(x).view(-1, d), target.view(-1))\n",
    "    loss_l2 = l2(model(x), target_onehot)\n",
    "    loss_l2_noisy = l2(model(x), noisy_target_onehot)\n",
    "    loss = loss_ce * loss_l2_noisy / loss_l2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"{d=}, {loss=}, {perf_counter() - t0:.3f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
